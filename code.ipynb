{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 255       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 5,305\n",
      "Trainable params: 5,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 118 samples, validate on 14 samples\n",
      "Epoch 1/40\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 1.6453 - acc: 0.2627 - val_loss: 1.7079 - val_acc: 0.2143\n",
      "Epoch 2/40\n",
      "118/118 [==============================] - 0s 325us/step - loss: 1.5996 - acc: 0.2627 - val_loss: 1.6618 - val_acc: 0.2143\n",
      "Epoch 3/40\n",
      "118/118 [==============================] - 0s 203us/step - loss: 1.4529 - acc: 0.3390 - val_loss: 1.6286 - val_acc: 0.0714\n",
      "Epoch 4/40\n",
      "118/118 [==============================] - 0s 211us/step - loss: 1.4570 - acc: 0.3814 - val_loss: 1.5921 - val_acc: 0.0714\n",
      "Epoch 5/40\n",
      "118/118 [==============================] - 0s 220us/step - loss: 1.4430 - acc: 0.3729 - val_loss: 1.5563 - val_acc: 0.1429\n",
      "Epoch 6/40\n",
      "118/118 [==============================] - 0s 228us/step - loss: 1.3627 - acc: 0.4576 - val_loss: 1.5244 - val_acc: 0.2143\n",
      "Epoch 7/40\n",
      "118/118 [==============================] - 0s 220us/step - loss: 1.3119 - acc: 0.5085 - val_loss: 1.4962 - val_acc: 0.2857\n",
      "Epoch 8/40\n",
      "118/118 [==============================] - 0s 228us/step - loss: 1.2197 - acc: 0.5339 - val_loss: 1.4662 - val_acc: 0.2857\n",
      "Epoch 9/40\n",
      "118/118 [==============================] - 0s 237us/step - loss: 1.2345 - acc: 0.5000 - val_loss: 1.4381 - val_acc: 0.2857\n",
      "Epoch 10/40\n",
      "118/118 [==============================] - 0s 228us/step - loss: 1.1453 - acc: 0.5932 - val_loss: 1.4104 - val_acc: 0.2857\n",
      "Epoch 11/40\n",
      "118/118 [==============================] - 0s 228us/step - loss: 1.0753 - acc: 0.6102 - val_loss: 1.3736 - val_acc: 0.2857\n",
      "Epoch 12/40\n",
      "118/118 [==============================] - 0s 237us/step - loss: 1.0657 - acc: 0.6525 - val_loss: 1.3218 - val_acc: 0.5000\n",
      "Epoch 13/40\n",
      "118/118 [==============================] - 0s 237us/step - loss: 0.9997 - acc: 0.7119 - val_loss: 1.2661 - val_acc: 0.5714\n",
      "Epoch 14/40\n",
      "118/118 [==============================] - 0s 228us/step - loss: 0.9507 - acc: 0.7119 - val_loss: 1.2201 - val_acc: 0.6429\n",
      "Epoch 15/40\n",
      "118/118 [==============================] - 0s 228us/step - loss: 0.9215 - acc: 0.7034 - val_loss: 1.1712 - val_acc: 0.7143\n",
      "Epoch 16/40\n",
      "118/118 [==============================] - 0s 228us/step - loss: 0.8580 - acc: 0.7627 - val_loss: 1.1272 - val_acc: 0.7143\n",
      "Epoch 17/40\n",
      "118/118 [==============================] - 0s 228us/step - loss: 0.7959 - acc: 0.7542 - val_loss: 1.0908 - val_acc: 0.6429\n",
      "Epoch 18/40\n",
      "118/118 [==============================] - 0s 228us/step - loss: 0.7190 - acc: 0.8305 - val_loss: 1.0608 - val_acc: 0.5714\n",
      "Epoch 19/40\n",
      "118/118 [==============================] - 0s 254us/step - loss: 0.7269 - acc: 0.7797 - val_loss: 1.0146 - val_acc: 0.7143\n",
      "Epoch 20/40\n",
      "118/118 [==============================] - 0s 237us/step - loss: 0.6862 - acc: 0.7712 - val_loss: 0.9794 - val_acc: 0.6429\n",
      "Epoch 21/40\n",
      "118/118 [==============================] - 0s 254us/step - loss: 0.6335 - acc: 0.7966 - val_loss: 0.9651 - val_acc: 0.5714\n",
      "Epoch 22/40\n",
      "118/118 [==============================] - 0s 245us/step - loss: 0.6439 - acc: 0.7966 - val_loss: 0.9216 - val_acc: 0.5714\n",
      "Epoch 23/40\n",
      "118/118 [==============================] - 0s 245us/step - loss: 0.5566 - acc: 0.8729 - val_loss: 0.8800 - val_acc: 0.6429\n",
      "Epoch 24/40\n",
      "118/118 [==============================] - 0s 228us/step - loss: 0.5218 - acc: 0.8729 - val_loss: 0.8925 - val_acc: 0.6429\n",
      "Epoch 25/40\n",
      "118/118 [==============================] - 0s 237us/step - loss: 0.5224 - acc: 0.8898 - val_loss: 0.8628 - val_acc: 0.6429\n",
      "Epoch 26/40\n",
      "118/118 [==============================] - 0s 237us/step - loss: 0.4586 - acc: 0.8983 - val_loss: 0.8669 - val_acc: 0.5714\n",
      "Epoch 27/40\n",
      "118/118 [==============================] - 0s 254us/step - loss: 0.4135 - acc: 0.9237 - val_loss: 0.8899 - val_acc: 0.5714\n",
      "Epoch 28/40\n",
      "118/118 [==============================] - 0s 245us/step - loss: 0.3909 - acc: 0.9237 - val_loss: 0.8277 - val_acc: 0.6429\n",
      "Epoch 29/40\n",
      "118/118 [==============================] - 0s 245us/step - loss: 0.3571 - acc: 0.9492 - val_loss: 0.8322 - val_acc: 0.5714\n",
      "Epoch 30/40\n",
      "118/118 [==============================] - 0s 220us/step - loss: 0.3420 - acc: 0.9322 - val_loss: 0.8831 - val_acc: 0.5714\n",
      "Epoch 31/40\n",
      "118/118 [==============================] - 0s 245us/step - loss: 0.2889 - acc: 0.9746 - val_loss: 0.8197 - val_acc: 0.7143\n",
      "Epoch 32/40\n",
      "118/118 [==============================] - 0s 245us/step - loss: 0.2744 - acc: 0.9831 - val_loss: 0.8066 - val_acc: 0.6429\n",
      "Epoch 33/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 0.2412 - acc: 1.000 - 0s 228us/step - loss: 0.2763 - acc: 0.9831 - val_loss: 0.8560 - val_acc: 0.5714\n",
      "Epoch 34/40\n",
      "118/118 [==============================] - 0s 228us/step - loss: 0.2377 - acc: 0.9831 - val_loss: 0.7884 - val_acc: 0.7143\n",
      "Epoch 35/40\n",
      "118/118 [==============================] - 0s 237us/step - loss: 0.2274 - acc: 0.9746 - val_loss: 0.7907 - val_acc: 0.7143\n",
      "Epoch 36/40\n",
      "118/118 [==============================] - 0s 228us/step - loss: 0.2000 - acc: 0.9915 - val_loss: 0.7930 - val_acc: 0.5714\n",
      "Epoch 37/40\n",
      "118/118 [==============================] - 0s 228us/step - loss: 0.1797 - acc: 0.9831 - val_loss: 0.7315 - val_acc: 0.7857\n",
      "Epoch 38/40\n",
      "118/118 [==============================] - 0s 228us/step - loss: 0.1595 - acc: 1.0000 - val_loss: 0.7136 - val_acc: 0.7143\n",
      "Epoch 39/40\n",
      "118/118 [==============================] - 0s 237us/step - loss: 0.1535 - acc: 1.0000 - val_loss: 0.7107 - val_acc: 0.6429\n",
      "Epoch 40/40\n",
      "118/118 [==============================] - 0s 237us/step - loss: 0.1615 - acc: 1.0000 - val_loss: 0.6951 - val_acc: 0.6429\n",
      "never talk to me again ðŸ˜ž\n",
      "i am proud of your achievements ðŸ˜\n",
      "it is the worst day in my life ðŸ˜ž\n",
      "miss you so much ðŸ’“\n",
      "food is life ðŸ´\n",
      "lets play chess âš¾\n",
      "i need some pizza and bread ðŸ´\n",
      "i am going to sleep ðŸ˜ž\n",
      "pleasure to meet you ðŸ˜ž\n",
      "thank you for the gift ðŸ´\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "emoji.EMOJI_ALIAS_UNICODE\n",
    "train = pd.read_csv('C:/Users/salma/Downloads/emoji project aman/emoji-prediction-master/train2.csv',header=None)\n",
    "test = pd.read_csv('C:/Users/salma/Downloads/emoji project aman/emoji-prediction-master/test2.csv',header=None)\n",
    "emoji_dict = {\n",
    "    0:\":beating_heart:\",\n",
    "    1:\":baseball:\",\n",
    "    2:\":beaming_face_with_smiling_eyes:\",\n",
    "    3:\":disappointed_face:\",\n",
    "    4:\":fork_and_knife:\"    \n",
    "}\n",
    "XT = train[0]\n",
    "YT = train[1]\n",
    "# Convert Sentence into List of Words\n",
    "def convertIntoList(sentences):\n",
    "    total  = sentences.shape[0]\n",
    "    X = []\n",
    "    for i in range(total):\n",
    "        X.append(sentences[i].lower().split())\n",
    "        \n",
    "    return X\n",
    "\n",
    "XT = convertIntoList(XT)\n",
    "\n",
    "YT = to_categorical(YT)\n",
    "def glove_vec(path):\n",
    "    import numpy as np\n",
    "    with open(path,'r',encoding='utf-8') as f:\n",
    "        words = set()\n",
    "        word_vec = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            key = line[0]\n",
    "            words.add(key)\n",
    "            val = np.array(line[1:],dtype=np.float64)\n",
    "            word_vec[key] = val\n",
    "            \n",
    "        # Dict (Vocab) --> set\n",
    "        word2idx = {}\n",
    "        idx2word = {}\n",
    "        \n",
    "        for i,w in enumerate(sorted(words)):\n",
    "            word2idx[w] = i\n",
    "            idx2word[i] = w\n",
    "            \n",
    "    return word_vec,word2idx,idx2word\n",
    "word_vec,word2idx,idx2word = glove_vec('glove.6B.50d.txt')\n",
    "def getEmbeddingOutput(X):\n",
    "    embedding_output = np.zeros((len(X),10,50))\n",
    "    #Iterate over every sentence and every word\n",
    "    for ix in range(len(X)):\n",
    "        for jx in range(len(X[ix])):\n",
    "            embedding_output[ix][jx] = word_vec[X[ix][jx]]\n",
    "            \n",
    "    return embedding_output\n",
    "XT_Processed = getEmbeddingOutput(XT)\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(50,input_shape=(10,50)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(XT_Processed,YT,epochs=40,validation_split=0.1)\n",
    "preds = np.argmax(model.predict(XT_Processed),axis=1)\n",
    "for i in range(5):\n",
    "    print(' '.join(XT[i]) + \" \" + emoji.emojize(emoji_dict[preds[i]]))\n",
    "my_test = np.array([\n",
    "    \"lets play chess\",\n",
    "    \"i need some pizza and bread\",\n",
    "    \"i am going to sleep\",\n",
    "    \"pleasure to meet you\",\n",
    "    \"thank you for the gift\"\n",
    "])\n",
    "out1 = convertIntoList(my_test)\n",
    "out2 = getEmbeddingOutput(out1)\n",
    "model.predict(out2)\n",
    "test_preds = np.argmax(model.predict(out2),axis=1)\n",
    "for i in range(5):\n",
    "    print(my_test[i] + \" \" + emoji.emojize(emoji_dict[test_preds[i]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "experience = \"miss you so much\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'miss you so much'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python aman.py --string \"miss you so much\"'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"python aman.py --string \\\"\" + experience + \"\\\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"python C:/Users/salma/Downloads/aman.py --string \\\"\" + experience + \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
